{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dc970fa-a645-48c4-a204-ef6ba3d70efe",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb233878-03b3-40f9-97cf-43db4d9ac153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page source after navigating to login: <html lang=\"en\" id=\"facebook\" class=\"\"><head><meta charset=\"utf-8\"><meta name=\"referrer\" content=\"origin-when-crossorigin\" id=\"meta_referrer\"><script nonce=\"\">function envFlush(a){function b(b){for(var c in a)b[c]=a[c]}window.requireLazy?window.requireLazy([\"Env\"],b):(window.Env=window.Env||{},b(window.Env))}envFlush({\"useTrustedTypes\":false,\"isTrustedTypesReportOnly\":false,\"ajaxpipe_token\":\"AXi7IGsvFxgBg3m0HiY\",\"stack_trace_limit\":30,\"timesliceBufferSize\":5000,\"show_invariant_decoder\":false,\"co\n",
      "Login failed: Message: \n",
      "\n",
      "An error occurred during scraping: HTTPConnectionPool(host='localhost', port=64766): Max retries exceeded with url: /session/10731433bc90d35f940c247ec191e8eb/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x10dcd0d90>: Failed to establish a new connection: [Errno 61] Connection refused'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/jk/wxfv5xn16_b00bdt6v49v7640000gn/T/ipykernel_3444/1969340579.py\", line 55, in facebook_login\n",
      "    WebDriverWait(driver, 20).until(EC.url_contains(\"facebook.com/home\"))\n",
      "  File \"/Users/itobuz/.conda/envs/final_project_env/lib/python3.10/site-packages/selenium/webdriver/support/wait.py\", line 146, in until\n",
      "    raise TimeoutException(message, screen, stacktrace)\n",
      "selenium.common.exceptions.TimeoutException: Message: \n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/itobuz/.conda/envs/final_project_env/lib/python3.10/site-packages/urllib3/connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"/Users/itobuz/.conda/envs/final_project_env/lib/python3.10/site-packages/urllib3/util/connection.py\", line 95, in create_connection\n",
      "    raise err\n",
      "  File \"/Users/itobuz/.conda/envs/final_project_env/lib/python3.10/site-packages/urllib3/util/connection.py\", line 85, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [Errno 61] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/itobuz/.conda/envs/final_project_env/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 716, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/Users/itobuz/.conda/envs/final_project_env/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 416, in _make_request\n",
      "    conn.request(method, url, **httplib_request_kw)\n",
      "  File \"/Users/itobuz/.conda/envs/final_project_env/lib/python3.10/site-packages/urllib3/connection.py\", line 244, in request\n",
      "    super(HTTPConnection, self).request(method, url, body=body, headers=headers)\n",
      "  File \"/Users/itobuz/.conda/envs/final_project_env/lib/python3.10/http/client.py\", line 1283, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"/Users/itobuz/.conda/envs/final_project_env/lib/python3.10/http/client.py\", line 1329, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"/Users/itobuz/.conda/envs/final_project_env/lib/python3.10/http/client.py\", line 1278, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/Users/itobuz/.conda/envs/final_project_env/lib/python3.10/http/client.py\", line 1038, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/Users/itobuz/.conda/envs/final_project_env/lib/python3.10/http/client.py\", line 976, in send\n",
      "    self.connect()\n",
      "  File \"/Users/itobuz/.conda/envs/final_project_env/lib/python3.10/site-packages/urllib3/connection.py\", line 205, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/Users/itobuz/.conda/envs/final_project_env/lib/python3.10/site-packages/urllib3/connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x10dcd0d90>: Failed to establish a new connection: [Errno 61] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/jk/wxfv5xn16_b00bdt6v49v7640000gn/T/ipykernel_3444/1969340579.py\", line 192, in scrape_facebook_posts\n",
      "    driver.get(PAGE_URL)\n",
      "  File \"/Users/itobuz/.conda/envs/final_project_env/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py\", line 454, in get\n",
      "    self.execute(Command.GET, {\"url\": url})\n",
      "  File \"/Users/itobuz/.conda/envs/final_project_env/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py\", line 427, in execute\n",
      "    response = self.command_executor.execute(driver_command, params)\n",
      "  File \"/Users/itobuz/.conda/envs/final_project_env/lib/python3.10/site-packages/selenium/webdriver/remote/remote_connection.py\", line 404, in execute\n",
      "    return self._request(command_info[0], url, body=data)\n",
      "  File \"/Users/itobuz/.conda/envs/final_project_env/lib/python3.10/site-packages/selenium/webdriver/remote/remote_connection.py\", line 428, in _request\n",
      "    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)\n",
      "  File \"/Users/itobuz/.conda/envs/final_project_env/lib/python3.10/site-packages/urllib3/request.py\", line 81, in request\n",
      "    return self.request_encode_body(\n",
      "  File \"/Users/itobuz/.conda/envs/final_project_env/lib/python3.10/site-packages/urllib3/request.py\", line 173, in request_encode_body\n",
      "    return self.urlopen(method, url, **extra_kw)\n",
      "  File \"/Users/itobuz/.conda/envs/final_project_env/lib/python3.10/site-packages/urllib3/poolmanager.py\", line 376, in urlopen\n",
      "    response = conn.urlopen(method, u.request_uri, **kw)\n",
      "  File \"/Users/itobuz/.conda/envs/final_project_env/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 830, in urlopen\n",
      "    return self.urlopen(\n",
      "  File \"/Users/itobuz/.conda/envs/final_project_env/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 830, in urlopen\n",
      "    return self.urlopen(\n",
      "  File \"/Users/itobuz/.conda/envs/final_project_env/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 830, in urlopen\n",
      "    return self.urlopen(\n",
      "  File \"/Users/itobuz/.conda/envs/final_project_env/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 802, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/Users/itobuz/.conda/envs/final_project_env/lib/python3.10/site-packages/urllib3/util/retry.py\", line 594, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=64766): Max retries exceeded with url: /session/10731433bc90d35f940c247ec191e8eb/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x10dcd0d90>: Failed to establish a new connection: [Errno 61] Connection refused'))\n"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Facebook Credentials\n",
    "FB_EMAIL = \"arnab@itobuz.com\"  # Replace with your Facebook email\n",
    "FB_PASSWORD = \"Arnab@2002\"     # Replace with your Facebook password\n",
    "# Facebook Page URL\n",
    "PAGE_URL = \"https://www.facebook.com/Itobuz/\"  # Replace with the target Facebook page URL\n",
    "# Folder to save posts and images\n",
    "SAVE_FOLDER = \"facebook_scraped_data\"\n",
    "os.makedirs(SAVE_FOLDER, exist_ok=True)\n",
    "\n",
    "# Set up Chrome WebDriver using `webdriver-manager`\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")  # Uncomment this line to enable headless mode\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36\")\n",
    "\n",
    "# Auto-download the correct ChromeDriver version\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def facebook_login():\n",
    "    \"\"\"\n",
    "    Logs into Facebook using provided credentials.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        driver.get(\"https://www.facebook.com/login\")\n",
    "        time.sleep(5)  # Add a delay to ensure the page loads\n",
    "        print(\"Page source after navigating to login:\", driver.page_source[:500])  # Debugging\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, \"email\")))\n",
    "        # Find and fill in the email field\n",
    "        email_element = driver.find_element(By.ID, \"email\")\n",
    "        email_element.send_keys(FB_EMAIL)\n",
    "        # Find and fill in the password field\n",
    "        password_element = driver.find_element(By.ID, \"pass\")\n",
    "        password_element.send_keys(FB_PASSWORD)\n",
    "        password_element.send_keys(Keys.RETURN)\n",
    "        # Wait for login to complete by checking for a known element on the homepage\n",
    "        WebDriverWait(driver, 20).until(EC.url_contains(\"facebook.com/home\"))\n",
    "        print(\"Logged into Facebook successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Login failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        driver.quit()\n",
    "        exit()\n",
    "\n",
    "def download_image(image_url, post_id, image_index):\n",
    "    \"\"\"\n",
    "    Downloads and saves an image from a given URL.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Skip invalid URLs (e.g., data:image/svg+xml)\n",
    "        if image_url.startswith(\"data:\"):\n",
    "            print(f\"Skipping invalid image URL: {image_url}\")\n",
    "            return None\n",
    "        # Ensure the URL is a complete URL\n",
    "        if image_url.startswith('/'):\n",
    "            image_url = \"https://www.facebook.com\" + image_url\n",
    "        # Make the request to download the image\n",
    "        response = requests.get(image_url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            image_path = os.path.join(SAVE_FOLDER, f\"{post_id}_{image_index}.jpg\")\n",
    "            with open(image_path, \"wb\") as file:\n",
    "                for chunk in response.iter_content(1024):\n",
    "                    file.write(chunk)\n",
    "            return image_path\n",
    "        else:\n",
    "            print(f\"Failed to download image. HTTP Status Code: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download image: {e}\")\n",
    "    return None\n",
    "\n",
    "def scroll_page():\n",
    "    \"\"\"\n",
    "    Scrolls down the page to load more posts dynamically.\n",
    "    \"\"\"\n",
    "    SCROLL_PAUSE_TIME = 5\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        print(\"Scrolling down...\")\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            print(\"No more content to load.\")\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "def click_see_more_buttons():\n",
    "    \"\"\"\n",
    "    Clicks all 'See more' buttons to reveal full post text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        see_more_buttons = driver.find_elements(By.XPATH, \"//div[contains(text(), 'See more')]\")\n",
    "        for button in see_more_buttons:\n",
    "            retries = 3\n",
    "            for _ in range(retries):\n",
    "                try:\n",
    "                    driver.execute_script(\"arguments[0].scrollIntoView();\", button)\n",
    "                    driver.execute_script(\"arguments[0].click();\", button)\n",
    "                    time.sleep(1)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"Retrying 'See more' click: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding 'See more' buttons: {e}\")\n",
    "\n",
    "def extract_hashtags(post):\n",
    "    \"\"\"\n",
    "    Extracts hashtags from the post by searching for anchor tags with href attributes starting with '/hashtag/'.\n",
    "    \"\"\"\n",
    "    hashtag_links = post.find_all(\"a\", href=lambda href: href and href.startswith(\"/hashtag/\"))\n",
    "    hashtags = [link.get_text(strip=True) for link in hashtag_links]\n",
    "    return hashtags\n",
    "\n",
    "def extract_emojis(post):\n",
    "    \"\"\"\n",
    "    Extracts emojis from the post by filtering <img> tags with emoji-specific attributes.\n",
    "    \"\"\"\n",
    "    emoji_images = post.find_all(\"img\", {\"class\": \"x1b0d499\"})  # Update class name if necessary\n",
    "    emojis = [img[\"alt\"] for img in emoji_images if \"alt\" in img.attrs]\n",
    "    return emojis\n",
    "\n",
    "def extract_post_data(post, index):\n",
    "    \"\"\"\n",
    "    Extracts data from a single post.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract post heading\n",
    "        heading_element = post.find(\"h3\")\n",
    "        post_heading = heading_element.get_text(strip=True) if heading_element else \"No heading available\"\n",
    "        \n",
    "        # Extract post text\n",
    "        text_element = post.find(\"div\", {\"data-ad-preview\": \"message\"})\n",
    "        post_text = text_element.get_text(strip=True) if text_element else \"No text available\"\n",
    "        \n",
    "        # Extract hashtags\n",
    "        hashtags = extract_hashtags(post)\n",
    "        \n",
    "        # Extract emojis\n",
    "        emojis = extract_emojis(post)\n",
    "        \n",
    "        # Extract image URLs (excluding emojis and invalid URLs)\n",
    "        image_elements = post.find_all(\"img\")\n",
    "        image_urls = [\n",
    "            img[\"src\"] for img in image_elements \n",
    "            if \"src\" in img.attrs and not img[\"src\"].startswith(\"data:\") and \"alt\" not in img.attrs\n",
    "        ]\n",
    "        local_image_paths = []\n",
    "        # Download images and save local paths\n",
    "        for img_index, image_url in enumerate(image_urls):\n",
    "            local_path = download_image(image_url, index, img_index)\n",
    "            if local_path:\n",
    "                local_image_paths.append(local_path)\n",
    "        \n",
    "        return {\n",
    "            \"heading\": post_heading,\n",
    "            \"text\": post_text,\n",
    "            \"hashtags\": hashtags,\n",
    "            \"emojis\": emojis,\n",
    "            \"image_urls\": image_urls,\n",
    "            \"local_image_paths\": local_image_paths if local_image_paths else [\"No images\"]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting post {index}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def scrape_facebook_posts():\n",
    "    \"\"\"\n",
    "    Scrapes posts and their images from a Facebook page after login.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        driver.get(PAGE_URL)\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"article\")))\n",
    "        print(\"Page loaded successfully.\")\n",
    "        \n",
    "        print(\"Scrolling to load more posts...\")\n",
    "        scroll_page()\n",
    "        \n",
    "        print(\"Clicking 'See more' buttons...\")\n",
    "        click_see_more_buttons()\n",
    "        \n",
    "        print(\"Extracting posts...\")\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        posts = soup.find_all(\"div\", {\"role\": \"article\"})\n",
    "        if not posts:\n",
    "            print(\"No posts found. Check selectors.\")\n",
    "            return\n",
    "        \n",
    "        posts_data = []\n",
    "        for index, post in enumerate(posts):\n",
    "            post_data = extract_post_data(post, index)\n",
    "            if post_data:\n",
    "                posts_data.append(post_data)\n",
    "        \n",
    "        # Save posts to a JSON file\n",
    "        json_path = os.path.join(SAVE_FOLDER, \"facebook_scraped_posts.json\")\n",
    "        with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(posts_data, f, indent=4)\n",
    "        print(f\"Posts saved successfully in {json_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during scraping: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Run the script\n",
    "facebook_login()\n",
    "scrape_facebook_posts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c501c4-93a0-48ad-8323-d5780094247c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
